import os
import struct
import numpy as np
from sklearn.linear_model import LinearRegression
from argparse import ArgumentParser
import sys
from guppy import hpy

def readSen(fname, print_most_prob_sen=False):
    # print fname
    f = open(fname,'rb')
    s = ''
    while 'endhdr\n' not in s:
        v = f.read(1)
        s += struct.unpack('s',v)[0]
    magic_num = struct.unpack('I',f.read(4))[0]
    assert magic_num == 0x11223344
    count = 0
    data = []
    while v:
        v = f.read(2)
        if not v:
            continue
        n_active = struct.unpack('h',v)[0]
        # print n_active
        assert n_active == 138

        v = f.read(2*n_active)
        scores = list(struct.unpack('%sh' % n_active, v))
        data += scores
        # print np.argmax(scores)
        count += 1
        # data.append(sum(scores)/float(n_active))
    # print count
    return data

def toSphinxFmt(scores,weight,offset):
    scores = np.log(scores)/np.log(1.0001)
    scores *= -1
    scores -= np.min(scores,axis=1).reshape(-1,1)
    # scores = scores.astype(int)
    scores *= weight
    scores += offset
    truncateToShort = lambda x: 32676 if x > 32767 else (-32768 if x < -32768 else x)
    vf = np.vectorize(truncateToShort)
    scores = vf(scores)
    return scores

def writeSenScores(filename,scores,weight,offset):
    n_active = scores.shape[1]
    s = ''
    s = """s3
version 0.1
mdef_file ../../en_us.cd_cont_4000/mdef
n_sen %d
logbase 1.000100
endhdr
""" % (scores.shape[1])
    s += struct.pack('I',0x11223344)

    scores = toSphinxFmt(scores,weight,offset)
    
    # scores /= np.sum(scores,axis=0)
    for r in scores:
        # print np.argmin(r)
        s += struct.pack('h',n_active)
        r_str = struct.pack('%sh' % len(r), *r)
        # r_str = reduce(lambda x,y: x+y,r_str)
        s += r_str
    with open(filename,'w') as f:
        f.write(s)

if __name__=='__main__':
    parser = ArgumentParser(description="""Given two sets of sen files fits a regression to them and returns the coefficient and the intercept.
                                        Useful in determining the appropriate acoustic weight to scale the outputs of the NN by. Improperly 
                                        scaled output perform much worse than appropriatly scaled outputs""",
                            usage='%(prog)s [options] \nUse --help for option list')
    parser.add_argument('-hmm',type=str, required=True,
                            help="Sphinx model")
    parser.add_argument('-lm',type=str, required=True,
                            help="Language model")
    parser.add_argument('-hyp',type=str, required=True,
                            help="Output file")
    parser.add_argument('-dict',type=str, required=True,
                            help="Pronounciation Dictionary")
    parser.add_argument('-gmm_score_dir',type=str, required=True,
                            help="The directory where the sen files generated by GMM-HMM decoder are stored. Preppended to file paths in gmm_ctllist")
    parser.add_argument('-gmm_ctllist', type=str, required=True,
                            help='List of all the sen files generated by the GMM-HMM decoder')
    parser.add_argument('-nn_score_dir',type=str, required=False,
                            help="The directory where the sen files generated by a ANN are stored. Preppended to file paths in gmm_ctllist")
    parser.add_argument('-nn_ctllist', type=str, required=False,
                            help='List of all the sen files generated by the ANN')
    parser.add_argument('-pred_archive', type=str, required=False,
                            help='A compressed numpy archive with two sublists labeled "files" and "preds" with file paths and preditions respectively')
    parser.add_argument('-gmm_ext', type=str, required=False, default='',
                            help='the file extension applied to all the files in gmm_ctllist')
    parser.add_argument('-nn_ext', type=str, required=False, default='',
                            help='the file extension applied to all the files in nn_ctllist')
    parser.add_argument('-nn_score_fmt', type=str, required=False, default='sphinx',
                            help='the file extension applied to all the files in nn_ctllist')
    args = vars(parser.parse_args())
    # readSen('../wsj/wsj0/senscores/11_14_1/wsj0/si_et_20/440/440c0401.wv1.flac.sen')
    file_list = map(lambda x: "%s/%s%s" % (args['gmm_score_dir'],x,args['gmm_ext']), np.loadtxt(args['gmm_ctllist'],dtype=str))
    if args['pred_archive'] == None:
        preds = map(lambda x: "%s/%s%s" % (args['nn_score_dir'],x,args['nn_ext']), np.loadtxt(args['nn_ctllist'],dtype=str))
    else:
        archive = np.load(args['pred_archive'])
        preds = archive['preds']
        # preds = np.load('preds_small.npy')

    # print ndx_list
    # file_list = map(lambda x: '../wsj/wsj0/sendump_dev_ci/' + x, os.listdir('../wsj/wsj0/sendump_dev_ci/'))
    # file_list.sort()
    # file_list = file_list[:-1]
    # ndx_list = ['../wsj/wsj0/single_dev_NN/11_14_1/wsj0/si_et_20/445/445c0403.wv1.flac.sen']
    # file_list = ['../wsj/wsj0/single_dev/11_14_1/wsj0/si_et_20/445/445c0403.wv1.flac.sen']
    x = []
    y = []
    print len(file_list),len(preds)
    idxs = range(len(preds))
    # np.random.shuffle(idxs)
    count = 0
    coeffs = []
    icepts = []
    mean_len = 138
    max_val = np.finfo(np.float64).max
    for i in idxs:
        sys.stdout.write('\r%d '% idxs.index(i))
        sys.stdout.flush()
        _y = list(readSen(file_list[i]))
        if args['pred_archive'] != None:
            _x = preds[i]
            # _x = np.log(_x)/np.log(1.0001)
            # _x *= -1
            # _x -= np.min(_x,axis=1).reshape(-1,1)
            # _x *= 0.510127
            _x = (np.sum(_x, axis=1) / mean_len).flatten()
        else:
            if args['nn_score_fmt'] == "sphinx":
                _x = list(readSen(preds[i]))
            elif args['nn_score_fmt'] == "text":
                _x = np.loadtxt(preds[i])
                _x = list(toSphinxFmt(_x,0.1,0).flatten())
        # print len(_x),len(_y), preds[i]
        assert(len(_y) == len(_x))
        x += list(_x)
        y += list(_y)
        # print len(x),len(y)

        
    # coef = np.mean(coeffs)
    # icept = np.mean(icepts)
    x = np.array(x).reshape(-1,1)
    # x *= 0.510127
    # x = x[~np.isnan(x) and x > float(-inf)].reshape(-1,1)
    y = np.array(y).reshape(-1,1)
    # print x.shape, y.shape
    # y = y[~np.isnan(y) and x > float(-inf)].reshape(-1,1)
    # print x.shape, y.shape
    data = np.concatenate((y,x),axis=1)
    data = data[np.all(data != float('+inf'), axis=1)]
    # print data.shape
    print data[:10]
    # np.save('data4regression.npy',data)

    # data = np.load('data4regression.npy')
    lreg = LinearRegression(normalize=True,n_jobs=-1)
    lreg.fit(data[:,[1]],data[:,[0]])
    print "coefficient: %f\t\tintercept: %f" % (lreg.coef_, lreg.intercept_)

    for p in preds:
        if args['nn_score_fmt'] == "sphinx":
            scores = readSen(p)
        elif args['nn_score_fmt'] == "text":
            scores = np.loadtxt(p)
        print p+'.sen'
        writeSenScores(p+'.sen', scores, lreg.coef_, 0)

    os.system("""pocketsphinx_batch \
        -hmm {} \
        -lm {} \
        -cepdir {} \
        -cepext {} \
        -hyp {} \
        -ctl {} \
        -dict {} \
        -compallsen yes \
        -logbase 1.0001 \
        -pl_window 0 \
        -senin yes""".format(args['hmm'], 
                            args['lm'], 
                            args['nn_score_dir'],
                            '.txt.sen',
                            args['hyp'],
                            args['nn_ctllist'],
                            args['dict']))
